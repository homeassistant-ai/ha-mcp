"""
Automation Lifecycle E2E Tests

Tests the complete automation workflow: Create ‚Üí Trigger ‚Üí Update ‚Üí Delete
This represents the most critical user journey for Home Assistant automation management.

Note: Tests are designed to work with both Docker test environment (localhost:8124)
and production environments. Entity references are dynamically discovered.
"""

import asyncio
import logging

import pytest

from ...utilities.assertions import (
    assert_mcp_success,
    parse_mcp_result,
    wait_for_automation,
)
from ...utilities.wait_helpers import (
    wait_for_condition,
    wait_for_entity_state,
    wait_for_logbook_entry,
)

logger = logging.getLogger(__name__)


@pytest.mark.automation
@pytest.mark.cleanup
class TestAutomationLifecycle:
    """Test complete automation management workflows."""

    async def _find_test_light_entity(self, mcp_client) -> str:
        """
        Find a suitable light entity for testing.

        Prefers demo entities, falls back to any available light.
        Returns entity_id of a suitable light for testing.
        """
        # Search for light entities
        search_result = await mcp_client.call_tool(
            "ha_search_entities",
            {"query": "light", "domain_filter": "light", "limit": 20},
        )

        search_data = parse_mcp_result(search_result)

        # Handle nested data structure
        if "data" in search_data:
            results = search_data.get("data", {}).get("results", [])
        else:
            results = search_data.get("results", [])

        if not results:
            pytest.skip("No light entities available for testing")

        # Prefer demo entities
        for entity in results:
            entity_id = entity.get("entity_id", "")
            if "demo" in entity_id.lower() or "test" in entity_id.lower():
                logger.info(f"üîç Using demo/test light: {entity_id}")
                return entity_id

        # Fall back to first available light
        entity_id = results[0].get("entity_id", "")
        if not entity_id:
            pytest.skip("No valid light entity found for testing")

        logger.info(f"üîç Using first available light: {entity_id}")
        return entity_id

    async def _find_test_binary_sensors(self, mcp_client) -> list[str]:
        """
        Find suitable binary sensor entities for testing.

        Returns list of binary sensor entity_ids suitable for testing.
        """
        # Search for binary sensor entities
        search_result = await mcp_client.call_tool(
            "ha_search_entities",
            {"query": "binary_sensor", "domain_filter": "binary_sensor", "limit": 20},
        )

        search_data = parse_mcp_result(search_result)

        # Handle nested data structure
        if "data" in search_data:
            results = search_data.get("data", {}).get("results", [])
        else:
            results = search_data.get("results", [])

        if not results:
            # If no binary sensors, use a light entity as fallback
            logger.warning("No binary_sensor entities found, using light as fallback")
            test_light = await self._find_test_light_entity(mcp_client)
            return [
                test_light,
                test_light,
            ]  # Return same entity twice for compatibility

        # Prefer demo entities
        demo_sensors = []
        all_sensors = []

        for entity in results:
            entity_id = entity.get("entity_id", "")
            if entity_id:
                all_sensors.append(entity_id)
                if "demo" in entity_id.lower() or "test" in entity_id.lower():
                    demo_sensors.append(entity_id)

        # Return at least 2 entities (duplicate if needed)
        if demo_sensors:
            result = demo_sensors[:2]
            if len(result) == 1:
                result.append(result[0])  # Duplicate if only one found
            logger.info(f"üîç Using demo binary sensors: {result}")
            return result

        if all_sensors:
            result = all_sensors[:2]
            if len(result) == 1:
                result.append(result[0])  # Duplicate if only one found
            logger.info(f"üîç Using available binary sensors: {result}")
            return result

        # Ultimate fallback - use light entities
        logger.warning(
            "No suitable binary sensors found, using light entities as fallback"
        )
        test_light = await self._find_test_light_entity(mcp_client)
        return [test_light, test_light]

    async def test_basic_automation_lifecycle(
        self, mcp_client, cleanup_tracker, test_data_factory
    ):
        """
        Test: Create basic automation ‚Üí Trigger ‚Üí Verify ‚Üí Delete

        This test validates the fundamental automation workflow that most
        users will follow when setting up Home Assistant automations.
        """

        # 1. DISCOVER: Find available test entities
        test_light = await self._find_test_light_entity(mcp_client)
        logger.info(f"üîç Using test light entity: {test_light}")

        # 2. CREATE: Basic time-based automation
        automation_name = "Morning Light E2E"
        create_config = test_data_factory.automation_config(
            automation_name,
            trigger=[{"platform": "time", "at": "07:00:00"}],
            action=[{"service": "light.turn_on", "target": {"entity_id": test_light}}],
        )

        logger.info(f"üìù Creating automation: {automation_name}")
        create_result = await mcp_client.call_tool(
            "ha_config_set_automation",
            { "config": create_config}
        )

        create_data = assert_mcp_success(create_result, "automation creation")

        # Extract automation entity ID with robust error handling
        automation_entity = create_data.get("entity_id")
        if not automation_entity:
            # Fallback to construct entity ID from alias
            alias = create_config["alias"]
            automation_entity = (
                f"automation.{alias.lower().replace(' ', '_').replace('_e2e', '_e2e')}"
            )
            logger.warning(
                f"No entity_id in response, using constructed ID: {automation_entity}"
            )

        # Validate entity ID format
        if not automation_entity.startswith("automation."):
            raise AssertionError(
                f"Invalid automation entity ID format: {automation_entity}"
            )

        cleanup_tracker.track("automation", automation_entity)
        logger.info(f"‚úÖ Created automation: {automation_entity}")

        # 3. VERIFY: Automation exists and is configured correctly
        # Wait for Home Assistant to register the new automation
        logger.info("üîç Verifying automation configuration...")
        config = await wait_for_automation(mcp_client, automation_entity, timeout=10)
        if not config:
            raise AssertionError(
                f"Automation {automation_entity} not found after creation"
            )

        # Check essential fields
        assert config.get("alias") == create_config["alias"], (
            f"Alias mismatch: {config.get('alias')} != {create_config['alias']}"
        )
        assert "trigger" in config or "triggers" in config, (
            "No triggers found in automation config"
        )
        assert "action" in config or "actions" in config, (
            "No actions found in automation config"
        )

        logger.info("‚úÖ Automation configuration verified")

        # 4. TRIGGER: Manually trigger the automation
        logger.info("üöÄ Triggering automation...")
        trigger_result = await mcp_client.call_tool(
            "ha_call_service",
            {
                "domain": "automation",
                "service": "trigger",
                "entity_id": automation_entity,
            },
        )

        trigger_data = assert_mcp_success(trigger_result, "automation trigger")
        logger.info("‚úÖ Automation triggered successfully")

        # 5. VERIFY: Check that automation ran (via logbook)
        logger.info("üìã Checking automation execution in logbook...")
        try:
            automation_logged = await wait_for_logbook_entry(
                mcp_client, automation_name, timeout=10, poll_interval=1.0
            )
            if automation_logged:
                logger.info("üìã Automation execution verified in logbook")
            else:
                logger.info(
                    "üìã Logbook verification timeout - automation trigger was successful"
                )
        except Exception as e:
            logger.warning(f"Logbook verification failed: {e} - continuing with test")

        # 6. UPDATE: Modify automation to add delay and different time
        logger.info("üìù Updating automation configuration...")
        update_config = test_data_factory.automation_config(
            f"{automation_name} Updated",
            trigger=[{"platform": "time", "at": "07:30:00"}],  # Different time
            action=[
                {"service": "light.turn_on", "target": {"entity_id": test_light}},
                {"delay": {"seconds": 2}},
                {"service": "light.turn_off", "target": {"entity_id": test_light}},
            ],
        )

        update_result = await mcp_client.call_tool(
            "ha_config_set_automation",
            {
                "identifier": automation_entity,
                "config": update_config},
        )

        update_data = assert_mcp_success(update_result, "automation update")
        logger.info("‚úÖ Automation updated successfully")

        # 7. VERIFY: Update was applied
        logger.info("üîç Verifying automation update...")
        config = await wait_for_automation(mcp_client, automation_entity, timeout=10)
        if not config:
            raise AssertionError(
                f"No configuration returned after update for automation {automation_entity}"
            )

        # Verify updated fields with better error messages
        assert config.get("alias") == update_config["alias"], (
            f"Alias not updated: {config.get('alias')} != {update_config['alias']}"
        )

        # Check actions (Home Assistant may return 'action' or 'actions')
        actions = config.get("actions") or config.get("action", [])
        assert len(actions) == 3, (
            f"Expected 3 actions after update, got {len(actions)}: {actions}"
        )

        # Check trigger time (Home Assistant may return 'trigger' or 'triggers')
        triggers = config.get("triggers") or config.get("trigger", [])
        if triggers:
            trigger_time = triggers[0].get("at")
            assert trigger_time == "07:30:00", (
                f"Trigger time not updated: {trigger_time} != 07:30:00"
            )

        logger.info("‚úÖ Automation update verified")

        # 8. DELETE: Clean up test automation
        logger.info("üóëÔ∏è Deleting automation...")
        delete_result = await mcp_client.call_tool(
            "ha_config_remove_automation",
            { "identifier": automation_entity},
        )

        delete_data = assert_mcp_success(delete_result, "automation deletion")
        logger.info("‚úÖ Automation deleted successfully")

        # 9. VERIFY: Automation is gone
        logger.info("üîç Verifying automation deletion...")
        # Poll to ensure deletion propagated (wait_for_automation returns None if not found)
        config = await wait_for_automation(mcp_client, automation_entity, timeout=5)

        # If still found, that's a problem
        if config is not None:
            raise AssertionError(
                f"Automation {automation_entity} still exists after deletion: {config}"
            )

        # Double-check with direct call for error message verification
        final_check = await mcp_client.call_tool(
            "ha_config_get_automation",
            { "identifier": automation_entity}
        )

        final_data = parse_mcp_result(final_check)
        # Automation should not exist anymore - this should fail
        assert not final_data.get("success"), (
            f"Automation should be deleted but still exists: {final_data}"
        )

        # Check for expected error indicators
        expected_errors = ["not found", "does not exist", "404"]
        error_msg = str(final_data.get("error", "")).lower()
        has_expected_error = any(err in error_msg for err in expected_errors)

        if final_data.get("success") or not has_expected_error:
            logger.warning(f"Unexpected deletion verification result: {final_data}")

        logger.info("‚úÖ Automation deletion verified")

    async def test_automation_enable_disable_lifecycle(
        self, mcp_client, cleanup_tracker, test_data_factory
    ):
        """
        Test: Automation enabling and disabling functionality

        This test validates automation state management which is critical
        for users who want to temporarily disable automations.
        """
        # Find test entity
        test_light = await self._find_test_light_entity(mcp_client)

        # Create automation in disabled state
        automation_name = "Toggle Test E2E"
        config = test_data_factory.automation_config(
            automation_name,
            trigger=[{"platform": "time", "at": "09:00:00"}],
            action=[{"service": "light.toggle", "target": {"entity_id": test_light}}],
            initial_state=False,  # Start disabled
        )

        logger.info(f"üìù Creating disabled automation: {automation_name}")
        create_result = await mcp_client.call_tool(
            "ha_config_set_automation",
            { "config": config}
        )

        create_data = assert_mcp_success(create_result, "disabled automation creation")
        automation_entity = (
            create_data.get("entity_id")
            or f"automation.{automation_name.lower().replace(' ', '_')}"
        )
        cleanup_tracker.track("automation", automation_entity)

        # Give HA a moment to process entity registration before polling
        await asyncio.sleep(5)

        # Wait for automation to be registered (existence only, not specific state)
        async def automation_exists():
            result = await mcp_client.call_tool("ha_get_state", {"entity_id": automation_entity})
            data = parse_mcp_result(result)
            return data.get("success", False)

        automation_ready = await wait_for_condition(
            automation_exists, timeout=20, condition_name=f"{automation_entity} registration"
        )
        assert automation_ready, f"Automation {automation_entity} not registered within timeout"
        logger.info("‚úÖ Automation correctly starts in disabled state")

        # Enable the automation
        logger.info("üîÑ Enabling automation...")
        enable_result = await mcp_client.call_tool(
            "ha_call_service",
            {
                "domain": "automation",
                "service": "turn_on",
                "entity_id": automation_entity,
            },
        )

        enable_data = assert_mcp_success(enable_result, "automation enable")

        # Verify automation is now enabled
        state_reached = await wait_for_entity_state(
            mcp_client, automation_entity, "on", timeout=20
        )
        assert state_reached, (
            f"Automation {automation_entity} did not reach enabled state 'on' within timeout"
        )
        logger.info("‚úÖ Automation successfully enabled")

        # Disable the automation
        logger.info("üîÑ Disabling automation...")
        disable_result = await mcp_client.call_tool(
            "ha_call_service",
            {
                "domain": "automation",
                "service": "turn_off",
                "entity_id": automation_entity,
            },
        )

        disable_data = assert_mcp_success(disable_result, "automation disable")

        # Verify automation is now disabled
        state_reached = await wait_for_entity_state(
            mcp_client, automation_entity, "off", timeout=20
        )
        assert state_reached, (
            f"Automation {automation_entity} did not reach disabled state 'off' within timeout"
        )
        logger.info("‚úÖ Automation successfully disabled")

        # Clean up
        delete_result = await mcp_client.call_tool(
            "ha_config_remove_automation",
            { "identifier": automation_entity},
        )
        assert_mcp_success(delete_result, "automation cleanup")
        logger.info("üóëÔ∏è Automation cleaned up")

    async def test_automation_yaml_validation(
        self, mcp_client, cleanup_tracker, test_data_factory
    ):
        """
        Test: Automation YAML configuration validation

        This test validates that automation configurations are properly validated
        and that invalid configurations are rejected appropriately.
        """
        test_light = await self._find_test_light_entity(mcp_client)

        # Test valid configuration
        logger.info("üß™ Testing valid automation configuration...")
        valid_config = test_data_factory.automation_config(
            "Valid Config E2E",
            trigger=[
                {"platform": "time", "at": "10:00:00"},
                {"platform": "state", "entity_id": test_light, "to": "on"},
            ],
            condition=[
                {"condition": "time", "after": "09:00:00", "before": "17:00:00"}
            ],
            action=[
                {"service": "light.turn_off", "target": {"entity_id": test_light}},
                {"delay": {"seconds": 5}},
                {
                    "service": "persistent_notification.create",
                    "data": {"message": "Valid automation executed", "title": "Test"},
                },
            ],
            mode="single",
        )

        create_result = await mcp_client.call_tool(
            "ha_config_set_automation",
            { "config": valid_config}
        )

        create_data = assert_mcp_success(create_result, "valid configuration creation")
        automation_entity = (
            create_data.get("entity_id")
            or f"automation.{valid_config['alias'].lower().replace(' ', '_')}"
        )
        cleanup_tracker.track("automation", automation_entity)
        logger.info("‚úÖ Valid configuration accepted")

        # Verify configuration structure
        get_result = await mcp_client.call_tool(
            "ha_config_get_automation",
            { "identifier": automation_entity}
        )

        get_data = assert_mcp_success(get_result, "configuration retrieval")
        config = get_data.get("config", {})

        # Validate all expected sections are present
        assert config.get("alias"), "Configuration missing alias"

        # Check triggers (Home Assistant may use 'trigger' or 'triggers')
        triggers = config.get("triggers") or config.get("trigger", [])
        assert len(triggers) == 2, f"Expected 2 triggers, got {len(triggers)}"

        # Check conditions
        conditions = config.get("conditions") or config.get("condition", [])
        assert len(conditions) == 1, f"Expected 1 condition, got {len(conditions)}"

        # Check actions
        actions = config.get("actions") or config.get("action", [])
        assert len(actions) == 3, f"Expected 3 actions, got {len(actions)}"

        # Check mode
        assert config.get("mode") == "single", (
            f"Expected mode 'single', got {config.get('mode')}"
        )

        logger.info("‚úÖ Configuration structure validated")

        # Test invalid configuration (should fail gracefully)
        logger.info("üß™ Testing invalid automation configuration...")
        invalid_config = {
            "alias": "Invalid Config E2E",
            "trigger": [
                {"platform": "invalid_platform"}
            ],  # Invalid: platform doesn't exist
            "action": [{"service": "nonexistent.service"}],  # Invalid service
        }

        try:
            invalid_result = await mcp_client.call_tool(
                "ha_config_set_automation",
                { "config": invalid_config}
            )

            invalid_data = parse_mcp_result(invalid_result)

            # Invalid config should fail
            if invalid_data.get("success"):
                logger.warning("Invalid configuration was unexpectedly accepted")
                # If it was accepted, clean it up
                if invalid_data.get("entity_id"):
                    cleanup_tracker.track("automation", invalid_data["entity_id"])
            else:
                logger.info("‚úÖ Invalid configuration properly rejected")

        except Exception as e:
            logger.info(
                f"‚úÖ Invalid configuration properly rejected with exception: {e}"
            )

        # Clean up valid automation
        delete_result = await mcp_client.call_tool(
            "ha_config_remove_automation",
            { "identifier": automation_entity},
        )
        assert_mcp_success(delete_result, "valid automation cleanup")
        logger.info("üóëÔ∏è Test automations cleaned up")

    @pytest.mark.slow
    async def test_complex_automation_with_conditions(
        self, mcp_client, cleanup_tracker, test_data_factory
    ):
        """
        Test: Complex automation with multiple triggers, conditions, and templates

        This test validates advanced automation features that power users rely on.
        """

        automation_name = "Complex Security E2E"

        # Discover test entities for complex automation
        test_light = await self._find_test_light_entity(mcp_client)
        test_binary_sensors = await self._find_test_binary_sensors(mcp_client)

        logger.info(
            f"üîç Using test entities - Light: {test_light}, Binary sensors: {test_binary_sensors}"
        )

        # Create complex automation with conditions and templates
        complex_config = test_data_factory.automation_config(
            automation_name,
            trigger=[
                {"platform": "state", "entity_id": test_binary_sensors[0], "to": "on"},
                {
                    "platform": "state",
                    "entity_id": (
                        test_binary_sensors[1]
                        if len(test_binary_sensors) > 1
                        else test_binary_sensors[0]
                    ),
                    "to": "on",
                },
            ],
            condition=[
                {"condition": "time", "after": "22:00:00", "before": "06:00:00"},
                {"condition": "state", "entity_id": test_light, "state": "off"},
            ],
            action=[
                {
                    "service": "light.turn_on",
                    "target": {"entity_id": test_light},
                    "data": {"brightness_pct": 25},
                },
                {
                    "service": "persistent_notification.create",
                    "data": {
                        "title": "Security Alert",
                        "message": "Activity detected at {{ now().strftime('%H:%M:%S') }}",
                    },
                },
            ],
            mode="single",
        )

        logger.info(f"üìù Creating complex automation: {automation_name}")
        create_result = await mcp_client.call_tool(
            "ha_config_set_automation",
            { "config": complex_config}
        )

        create_data = assert_mcp_success(create_result, "complex automation creation")

        automation_entity = (
            create_data.get("entity_id")
            or f"automation.{automation_name.lower().replace(' ', '_')}"
        )
        if not automation_entity.startswith("automation."):
            raise AssertionError(
                f"Invalid complex automation entity ID format: {automation_entity}"
            )

        cleanup_tracker.track("automation", automation_entity)
        logger.info(f"‚úÖ Complex automation created: {automation_entity}")

        # Test template evaluation used in the automation
        logger.info("üß™ Testing template evaluation...")
        template_result = await mcp_client.call_tool(
            "ha_eval_template", {"template": "{{ now().strftime('%H:%M:%S') }}"}
        )

        template_data = assert_mcp_success(template_result, "template evaluation")

        result = template_data.get("result", "")
        assert ":" in result, (
            f"Template should return time string with colon, got: {result}"
        )
        assert len(result) >= 8, f"Template result too short for time format: {result}"

        logger.info(f"‚úÖ Template evaluation works: {result}")

        # Verify complex configuration
        logger.info("üîç Verifying complex automation configuration...")
        get_result = await mcp_client.call_tool(
            "ha_config_get_automation",
            { "identifier": automation_entity}
        )

        get_data = assert_mcp_success(get_result, "complex automation retrieval")

        config = get_data.get("config", {})
        if not config:
            raise AssertionError(
                f"No configuration returned for complex automation {automation_entity}"
            )

        # Home Assistant API returns plural forms
        triggers = config.get("triggers") or config.get("trigger", [])
        conditions = config.get("conditions") or config.get("condition", [])
        actions = config.get("actions") or config.get("action", [])

        # Validate configuration structure
        assert len(triggers) == 2, (
            f"Expected 2 triggers, got {len(triggers)}: {triggers}"
        )
        assert len(conditions) == 2, (
            f"Expected 2 conditions, got {len(conditions)}: {conditions}"
        )
        assert len(actions) == 2, f"Expected 2 actions, got {len(actions)}: {actions}"
        assert config.get("mode") == "single", (
            f"Expected mode 'single', got: {config.get('mode')}"
        )

        logger.info("‚úÖ Complex automation configuration verified")

        # Cleanup
        logger.info("üóëÔ∏è Cleaning up complex automation...")
        delete_result = await mcp_client.call_tool(
            "ha_config_remove_automation",
            { "identifier": automation_entity},
        )

        delete_data = assert_mcp_success(delete_result, "complex automation deletion")
        logger.info("‚úÖ Complex automation cleaned up")

    async def test_automation_mode_behaviors(
        self, mcp_client, cleanup_tracker, test_data_factory
    ):
        """
        Test: Different automation execution modes (single, restart, queued, parallel)

        This test validates automation execution behavior modes that affect how
        automations handle multiple triggers.
        """

        # Test different mode configurations
        modes_to_test = ["single", "restart", "queued", "parallel"]

        for mode in modes_to_test:
            automation_name = f"Mode Test {mode.title()} E2E"
            logger.info(f"üß™ Testing automation mode: {mode}")

            # Use dynamic test entity
            test_light = await self._find_test_light_entity(mcp_client)

            mode_config = test_data_factory.automation_config(
                automation_name,
                trigger=[{"platform": "time", "at": "08:00:00"}],
                action=[
                    {"delay": {"seconds": 1}},
                    {"service": "light.toggle", "target": {"entity_id": test_light}},
                ],
                mode=mode,
                max=3 if mode in ["queued", "parallel"] else None,
            )

            # Remove None values
            if mode_config.get("max") is None:
                mode_config.pop("max", None)

            create_result = await mcp_client.call_tool(
                "ha_config_set_automation",
                { "config": mode_config}
            )

            create_data = assert_mcp_success(
                create_result, f"{mode} mode automation creation"
            )

            automation_entity = (
                create_data.get("entity_id")
                or f"automation.{automation_name.lower().replace(' ', '_')}"
            )
            if not automation_entity.startswith("automation."):
                raise AssertionError(
                    f"Invalid {mode} automation entity ID format: {automation_entity}"
                )

            cleanup_tracker.track("automation", automation_entity)

            # Verify mode is set correctly
            get_result = await mcp_client.call_tool(
                "ha_config_get_automation",
                { "identifier": automation_entity},
            )

            get_data = assert_mcp_success(
                get_result, f"{mode} mode automation retrieval"
            )

            config = get_data.get("config", {})
            if not config:
                raise AssertionError(
                    f"No configuration returned for {mode} automation {automation_entity}"
                )

            assert config.get("mode") == mode, (
                f"Mode not set correctly for {mode}: expected '{mode}', got '{config.get('mode')}'"
            )

            if mode in ["queued", "parallel"]:
                max_value = config.get("max")
                assert max_value == 3, (
                    f"Max not set correctly for {mode}: expected 3, got {max_value}"
                )

            logger.info(f"‚úÖ Mode {mode} automation created and verified")

            # Cleanup immediately to avoid entity ID conflicts
            delete_result = await mcp_client.call_tool(
                "ha_config_remove_automation",
                { "identifier": automation_entity},
            )

            delete_data = assert_mcp_success(
                delete_result, f"{mode} mode automation deletion"
            )
            logger.info(f"üóëÔ∏è Mode {mode} automation cleaned up")


@pytest.mark.automation
async def test_automation_search_and_discovery(mcp_client):
    """
    Test: Automation search and discovery capabilities

    Validates that users can find and explore existing automations
    through the search functionality.
    """

    logger.info("üîç Testing automation search and discovery...")

    # Search for existing automations
    search_result = await mcp_client.call_tool(
        "ha_search_entities",
        {"query": "automation", "domain_filter": "automation", "limit": 10},
    )

    search_data = parse_mcp_result(search_result)

    # Handle different response formats
    if "data" in search_data:
        # Success is nested in data
        data_section = search_data.get("data", {})
        assert data_section.get("success"), f"Automation search failed: {search_data}"
        results = data_section.get("results", [])
    else:
        # Success is at top level
        assert search_data.get("success"), f"Automation search failed: {search_data}"
        results = search_data.get("results", [])

    logger.info(f"üîç Found {len(results)} automations")

    # Get system overview to see automation status
    overview_result = await mcp_client.call_tool("ha_get_overview")
    overview_data = parse_mcp_result(overview_result)

    # Should have automation information in overview
    overview_text = str(overview_data).lower()
    assert "automation" in overview_text, (
        "System overview should include automation information"
    )
    logger.info("‚úÖ System overview includes automation data")

    # Test entity search with different patterns
    search_patterns = ["morning", "light", "security"]
    for pattern in search_patterns:
        pattern_result = await mcp_client.call_tool(
            "ha_search_entities",
            {"query": pattern, "domain_filter": "automation", "limit": 5},
        )

        pattern_data = parse_mcp_result(pattern_result)

        # Handle nested data structure if present
        if "data" in pattern_data:
            results = pattern_data.get("data", {}).get("results", [])
        else:
            results = pattern_data.get("results", [])

        logger.info(f"üîç Pattern '{pattern}' search: {len(results)} results")

    logger.info("‚úÖ Automation search and discovery tests completed")

# Open WebUI + Ollama + MCP Testing Stack
# Tests multiple configurations to diagnose tool calling issues with local LLMs.
#
# Usage:
#   1. Copy .env.example to .env and fill in your HA credentials
#   2. docker compose up -d
#   3. Wait for model download: docker compose logs -f ollama
#   4. Open http://localhost:3000
#
# Test Strategy:
#   Step 1: Test mini-mcp (3 simple tools) - verifies model can call tools at all
#   Step 2: Test ha-mcp via mcpo (OpenAPI) - verifies OpenAPI bridge works
#   Step 3: Test ha-mcp native MCP - verifies native MCP protocol works
#
# Endpoints:
#   - Open WebUI: http://localhost:3000
#   - MCPO (mini-mcp): http://localhost:8001/mini-mcp/docs
#   - MCPO (ha-mcp): http://localhost:8000/ha-mcp/docs
#   - mini-mcp SSE: http://localhost:8087/sse
#   - ha-mcp SSE: http://localhost:8086/mcp
#
# Debugging:
#   docker compose logs -f mini-mcp
#   docker compose logs -f mcpo-mini
#   docker compose logs -f ha-mcp
#   docker compose logs -f mcpo-ha

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    volumes:
      - ollama_data:/root/.ollama
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_DEBUG=1
    restart: unless-stopped
    # Uncomment for NVIDIA GPU support:
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]

  # Pull multiple models to test different tool-calling capabilities
  ollama-init:
    image: curlimages/curl:latest
    container_name: ollama-init
    depends_on:
      - ollama
    restart: "no"
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        echo "Waiting for Ollama to be ready..."
        until curl -sf http://ollama:11434/api/tags > /dev/null 2>&1; do
          echo "Ollama not ready yet..."
          sleep 2
        done
        echo "Ollama is ready! Pulling models with tool calling support..."

        # Pull models known to support tool calling
        echo "Pulling qwen2.5:7b (good tool calling)..."
        curl -X POST http://ollama:11434/api/pull -d '{"name":"qwen2.5:7b"}'

        echo "Pulling llama3.1:8b..."
        curl -X POST http://ollama:11434/api/pull -d '{"name":"llama3.1:8b"}'

        echo "Models ready!"

  # ========================================
  # MINI MCP - Simple test server (3 tools)
  # ========================================
  # Runs via MCPO which handles the MCP protocol
  mcpo-mini:
    image: ghcr.io/open-webui/mcpo:main
    container_name: mcpo-mini
    ports:
      - "8001:8000"
    volumes:
      - ./mini_mcp.py:/app/mini_mcp.py:ro
    command: ["--port", "8000", "--name", "mini-mcp", "--", "uvx", "--from", "fastmcp", "fastmcp", "run", "/app/mini_mcp.py"]
    restart: unless-stopped

  # ========================================
  # HA-MCP - Full Home Assistant MCP
  # ========================================
  ha-mcp:
    image: ghcr.io/homeassistant-ai/ha-mcp:latest
    container_name: ha-mcp
    command: ["python", "-u", "-c", "import logging; logging.basicConfig(level=logging.DEBUG, format='%(asctime)s %(name)s %(levelname)s: %(message)s'); from ha_mcp.__main__ import main_web; main_web()"]
    ports:
      - "8086:8086"
    environment:
      - HOMEASSISTANT_URL=${HOMEASSISTANT_URL}
      - HOMEASSISTANT_TOKEN=${HOMEASSISTANT_TOKEN}
      # Limit tools to reduce complexity for local models
      - ENABLED_TOOL_MODULES=${ENABLED_TOOL_MODULES:-}
    restart: unless-stopped

  # MCPO bridge for ha-mcp (OpenAPI endpoint)
  # Test at: http://localhost:8000/ha-mcp/docs
  mcpo-ha:
    image: ghcr.io/open-webui/mcpo:main
    container_name: mcpo-ha
    ports:
      - "8000:8000"
    command: ["--port", "8000", "--name", "ha-mcp", "--", "uvx", "ha-mcp@latest"]
    environment:
      - HOMEASSISTANT_URL=${HOMEASSISTANT_URL}
      - HOMEASSISTANT_TOKEN=${HOMEASSISTANT_TOKEN}
      - ENABLED_TOOL_MODULES=${ENABLED_TOOL_MODULES:-}
    restart: unless-stopped

  # ========================================
  # OPEN WEBUI
  # ========================================
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    ports:
      - "3000:8080"
    volumes:
      - open_webui_data:/app/backend/data
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      # Pre-configure tool servers
      # Start with mini-mcp for testing (simplest)
      - TOOL_SERVER_CONNECTIONS=[{"url":"http://mcpo-mini:8000/mini-mcp","path":"","type":"openapi","auth_type":"none","key":"","config":{"enable":true},"info":{"id":"mini-mcp","name":"Mini Test MCP","description":"Simple test tools (get_time, add_numbers, greet)"}}]
      - DEFAULT_MODELS=qwen2.5:7b
      - WEBUI_LOG_LEVEL=DEBUG
      - WEBUI_AUTH=false
    depends_on:
      - ollama
      - mcpo-mini
    restart: unless-stopped

volumes:
  ollama_data:
  open_webui_data:

name: Performance Tests

on:
  push:
    branches: [ main, master ]
    paths:
      # Only run on code changes that could affect performance
      - 'src/**'
      - 'tests/src/e2e/performance/**'
      - 'tests/src/e2e/utilities/performance.py'
      - '.github/workflows/performance-tests.yml'
  pull_request:
    branches: [ master ]
    paths:
      - 'src/**'
      - 'tests/src/e2e/performance/**'
      - 'tests/src/e2e/utilities/performance.py'
      - '.github/workflows/performance-tests.yml'
  workflow_dispatch:
  # Run weekly to catch performance regressions
  schedule:
    - cron: '0 6 * * 1'  # Every Monday at 6 AM UTC

permissions:
  contents: read

env:
  PYTHON_VERSION: "3.13"
  UV_CACHE_DIR: /tmp/.uv-cache

jobs:
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20

    steps:
    - uses: actions/checkout@v6

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Install uv
      uses: astral-sh/setup-uv@v7
      with:
        version: "latest"

    - name: Set up Python
      run: uv python install ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: uv sync --all-extras --dev

    - name: Run performance tests
      id: perf_tests
      run: |
        echo "Running performance tests..."
        uv run pytest tests/src/e2e/performance/ \
          -v \
          --tb=short \
          -m performance \
          --log-cli-level=INFO \
          2>&1 | tee performance_output.txt

        # Capture exit code
        exit_code=${PIPESTATUS[0]}
        echo "exit_code=$exit_code" >> $GITHUB_OUTPUT
        exit $exit_code
      env:
        HAMCP_ENV_FILE: "tests/.env.test"

    - name: Upload performance results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: performance-results
        path: performance_output.txt
        retention-days: 30

    - name: Comment on PR with performance results
      if: github.event_name == 'pull_request' && always()
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          let output = '';
          try {
            output = fs.readFileSync('performance_output.txt', 'utf8');
          } catch (e) {
            output = 'Performance test output not available';
          }

          // Extract timing information from output
          const timingLines = output.split('\n')
            .filter(line => line.includes('avg=') || line.includes('p95=') || line.includes('SLOW') || line.includes('OK'))
            .slice(0, 20);

          const status = '${{ steps.perf_tests.outputs.exit_code }}' === '0' ? 'passed' : 'failed';
          const emoji = status === 'passed' ? ':white_check_mark:' : ':x:';

          const body = `## Performance Test Results ${emoji}

          **Status:** ${status}

          ### Timing Summary
          \`\`\`
          ${timingLines.join('\n') || 'No timing data available'}
          \`\`\`

          <details>
          <summary>Full Output</summary>

          \`\`\`
          ${output.slice(-5000)}
          \`\`\`
          </details>
          `;

          // Find existing comment or create new
          const { data: comments } = await github.rest.issues.listComments({
            owner: context.repo.owner,
            repo: context.repo.repo,
            issue_number: context.issue.number,
          });

          const botComment = comments.find(c =>
            c.user.type === 'Bot' && c.body.includes('Performance Test Results')
          );

          if (botComment) {
            await github.rest.issues.updateComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              comment_id: botComment.id,
              body: body
            });
          } else {
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: body
            });
          }
